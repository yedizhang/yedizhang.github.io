<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:28px;
		font-weight:400;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #2268c3;
		text-decoration: none;
	}
	a:hover {
		color: #2268c3;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>


<html>
<head>
	<title>Understanding Unimodal Bias in Multimodal Deep Linear Networks</title>
	<meta property="og:image" content="./img/unimodal-bias/L2_late_feat.gif"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Understanding Unimodal Bias in Multimodal Deep Linear Networks" />
	<meta property="og:description" content="We study multimodal deep linear networks and show that late and intermediate fusion architectures can give rise to long unimodal phases and permanent unimodal bias." />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EJGLRC78GK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EJGLRC78GK');
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
</head>

<body>
	<br>
	<center>
		<span style="font-size:32px;font-weight:400">Understanding Unimodal Bias in Multimodal Deep Linear Networks</span>	
		<table align=center width=600px>
			<table align=center width=600px>
				<tr><br></tr>
				<tr><span style="font-size:20px;font-weight:400">ICML 2024</span></tr>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://yedizhang.github.io/">Yedi Zhang</a><sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://www.gatsby.ucl.ac.uk/~pel/">Peter Latham</a><sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://www.saxelab.org/people/andrewsaxe/">Andrew Saxe</a><sup>1,2</sup></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<center>
						1: Gatsby Computational Neuroscience Unit, University College London <br>
						2: Sainsbury Wellcome Centre, University College London
					</center>
				</tr>
			</table>
		</table>
	</center>
	
	<br><hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Using multiple input streams simultaneously to train multimodal neural networks is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where a network overly relies on one modality and ignores others during joint training. We develop a theory of unimodal bias with deep multimodal linear networks to understand how architecture and data statistics influence this bias. This is the first work to calculate the duration of the unimodal phase in learning as a function of the depth at which modalities are fused within the network, dataset statistics, and initialization. We show that the deeper the layer at which fusion occurs, the longer the unimodal phase. A long unimodal phase can lead to a generalization deficit and permanent unimodal bias in the overparametrized regime. Our results, derived for multimodal linear networks, extend to ReLU networks in certain settings. Taken together, this work illuminates pathologies of multimodal learning under joint training, showing that late and intermediate fusion architectures can give rise to long unimodal phases and permanent unimodal bias.
			</td>
		</tr>
	</table>
	
	<br>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./img/unimodal-bias/L2_early_feat.gif"/>
					</center>
				</td>
			</tr>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./img/unimodal-bias/L2_late_feat.gif"/>
					</center>
				</td>
			</tr>

		</table>
		<table align=center width=850px>
			<tr>
				<td>
					<center>
						Loss and weights trajectories of early fusion (upper row) and late fusion (lower row) linear networks.
					</center>
				</td>
			</tr>
		</table>
	</center>

	<br><hr>

	<table align=center width=850px>
		<center><h1>Supplementary Material</h1></center>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="https://openreview.net/attachment?id=ul1cjLB98Y&name=supplementary_material">[zip]</a>
			</center></td>
		</tr>
		<tr>
			<td>
				Videos of feature evolution corresponding to figures of training trajectories shown in the paper.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<center>
					This webpage template is borrowed from <a href="https://github.com/richzhang/webpage-template">here</a>.
				</center>
			</td>
		</tr>
	</table>

</body>
</html>
